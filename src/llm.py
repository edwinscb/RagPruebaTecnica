import ollama
from search import search

def generate_response(query):
    """Genera una respuesta basada en los fragmentos recuperados."""
    results = search(query, top_k=5)  # Buscar en la base de datos

    if not results:  # Verificar si `results` es None o vacÃ­o
        return "âš ï¸ No se encontraron documentos relevantes para la consulta."

    context = "\n\n".join([f"- {contenido_texto}" for _, _, contenido_texto in results])
    print(f"Chunks obtenidos:\n{context}")
    prompt = f"""
    Eres un asistente en espaÃ±ol que responde preguntas basÃ¡ndose en el siguiente contexto:

    {context}

    Pregunta: {query}
    Respuesta en espaÃ±ol:
    """

    response = ollama.chat(model="llama3.2:3b", messages=[{"role": "user", "content": prompt}])
    return response['message']['content']

if __name__ == "__main__":
    consulta = input("ğŸ” Ingrese su consulta: ")
    respuesta = generate_response(consulta)
    
    print("\nğŸ¤– Respuesta del LLM:\n", respuesta)
